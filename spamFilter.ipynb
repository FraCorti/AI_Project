{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spamFilter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FraCorti/AI_Project/blob/master/spamFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms1cN3GnG-2c",
        "colab_type": "text"
      },
      "source": [
        "Added **Kaggle** installation and downloading *spam.csv* dataset from the following link: [spam-collection-dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a61Iy7zuE9ft",
        "colab_type": "code",
        "outputId": "1263cc30-ea28-4893-fce7-857b46c1f6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir /root/.kaggle\n",
        "!echo '{\"username\":\"fcorti\",\"key\":\"fafa72652d9ae6ee3bab07bad5a1a96c\"}' > /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d uciml/sms-spam-collection-dataset\n",
        "!pip install pandas"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading sms-spam-collection-dataset.zip to /content\n",
            "  0% 0.00/208k [00:00<?, ?B/s]\n",
            "100% 208k/208k [00:00<00:00, 63.5MB/s]\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoVpBVaDs38q",
        "colab_type": "text"
      },
      "source": [
        "Imported **pandas** and convert the *sms-spam-collection-dataset.zip* file into *.csv* format\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZnbzUq_MrUJ",
        "colab_type": "code",
        "outputId": "108bf5ed-071b-434f-a8ab-c095687b5db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "zip_file = ZipFile('/content/sms-spam-collection-dataset.zip')\n",
        "fields= ['v1','v2'] \n",
        "data=pd.read_csv(zip_file.open('spam.csv'), usecols=fields, encoding = \"latin-1\")\n",
        "print(\"Dataset shape: \", data.shape)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset shape:  (5572, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye8jl8YJjE7x",
        "colab_type": "text"
      },
      "source": [
        "Import of  **Keras** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlszU0L2jFa8",
        "colab_type": "code",
        "outputId": "fe25b035-0296-4172-e522-f1506d20e007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "# num_max = 1000\n",
        "# le = LabelEncoder()\n",
        "# tags = le.fit_transform(tags)\n",
        "# tok = Tokenizer(num_words=num_max)\n",
        "# tok.fit_on_texts(texts)\n",
        "# mat_texts = tok.texts_to_matrix(texts,mode='count')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# x_train,x_test,y_train,y_test = train_test_split(texts,tags, test_size = 0.2)\n",
        "# mat_texts_tr = tok.texts_to_matrix(x_train,mode='count')\n",
        "# mat_texts_tst = tok.texts_to_matrix(x_test,mode='count')\n",
        "\n",
        "# max_len = 100\n",
        "# x_train = tok.texts_to_sequences(x_train)\n",
        "# x_test = tok.texts_to_sequences(x_test)\n",
        "# cnn_texts_mat = sequence.pad_sequences(x_train,maxlen=max_len)\n",
        "# max_len = 100\n",
        "# cnn_texts_mat_tst = sequence.pad_sequences(x_test,maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e2ba11739cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tags' is not defined"
          ]
        }
      ]
    }
  ]
}